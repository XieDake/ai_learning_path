# 1. 目标检测组件-主干网架构
> 本系列内容基本来自目标检测综述论文《Recent Advances in Deep Learning for Object Detection》\
> 下载地址：https://arxiv.org/pdf/1908.03673.pdf

&emsp;&emsp;R-CNN表明采用大规模图像分类问题预训练的模型中的卷积权重可以为训练检测器提供更丰富的语义信息，并提高检测性能。 在后来的几年中，这种方法已成为大多数目标检测器的默认策略。 在本节中，我们将首先简要介绍深度卷积神经网络的基本概念，然后回顾一些广泛用于检测的体系结构。

## 1.1. CNN的基础架构

&emsp;&emsp;深度卷积神经网络（Deep convolutional neural network，DCNN）是典型的深度神经网络，并已被证明在视觉理解方面极为有效。深度卷积神经网络通常由一系列卷积层，池化层，非线性激活层和完全连接层（FC层）组成。 卷积层接受图像输入，并通过n×n个核对其进行卷积以生成特征图。所生成的特征图可以被视为多通道图像，并且每个通道代表该图像的不同信息。特征图中的每个像素（称为神经元）都连接到前一个图中相邻神经元的一小部分，这称为感受野(receptive field)。 生成特征图后，将应用非线性激活层。池化层用于整合感受野内的信号，用以扩大感受野并降低计算成本。

&emsp;&emsp;通过一系列卷积层，池化层和非线性激活层的组合，构建了深度卷积神经网络。整个网络可以通过基于梯度的优化方法（SGD，Adam等）通过定义的损失函数进行优化。经典的卷积神经网络AlexNet，包含五个卷积层， 三个最大池化层和三个完全连接层。 每个卷积层之后是ReLU非线性激活层。

## 1.2. 目标检测中的CNN主干网络
&emsp;&emsp;通过一系列卷积层，池化层和非线性激活层的组合，构建了深度卷积神经网络。整个网络可以通过基于梯度的优化方法（SGD，Adam等）通过定义的损失函数进行优化。经典的卷积神经网络AlexNet，包含五个卷积层，在本节中，我们将回顾一些具有最新结果的，广泛用于目标检测任务的体系结构，例如VGG16、ResNet、ResNeXt和Hourglass。

### 1.2.1. VGG16
&emsp;&emsp;通过一系列卷积层，池化层和非线性激活层的组合，构建了深度卷积神经网络。整个网络可以通过基于梯度的优化方法（SGD，Adam等）通过定义的损失函数进行优化。经典的卷积神经网络AlexNet，包含五个卷积层，VGG16是基于AlexNet开发的。 VGG16由五组卷积层和三个FC层组成。前两组中有两个卷积层，后三组中有三个卷积层。在每个组之间，应用一个最大池化层以减小空间尺寸。 VGG16表明，通过堆叠卷积层来增加网络深度可以提高模型的表达能力，并带来更好的性能。但是，仅通过堆叠卷积层将模型深度增加到20层，就导致了SGD的优化挑战。即使在训练阶段，性能也明显下降并且不如较浅的模型。

### 1.2.2. ResNet
&emsp;&emsp;基于上述观察，He等提出了ResNet，它通过引入快捷连接（shortcut connections）减少了优化难度。在这里，一层可以跳过非线性变换，直接将值直接传递给下一层。给出为：

$$x_{l + 1} = x_l + f_{l + 1}(x_l，θ)$$

其中 $x_l$ 是第l层的输入特征，而 $f_{l + 1}$ 表示对输入 $x_l$ 的操作，例如卷积，归一化或非线性激活。 $f_{l + 1}(x_l,θ)$ 是 $x_l$ 的残差函数，因此任何深层的特征图都可以看作是浅层激活和残差函数的总和。快捷连接创建了一条高速公路，该高速公路将梯度计算从深层单元直接传播到浅层单元，因此大大降低了训练难度。利用残差块能够有效地训练网络，模型深度可以增加（例如从16增加到152），从而使我们能够训练非常大容量的模型。后来，He等提出了ResNet的预激活变体，名为ResNet-v2。他们的实验表明，适当的批归一化排序可能会比原始ResNet更好。对ResNet的这种简单而有效的修改使训练超过1000层的网络成为可能，并且由于深度的增加，仍然可以享受改进的性能。

### 1.2.3. DenseNet
&emsp;&emsp;Huang等人认为，尽管ResNet通过快捷连接减少了训练难度，但它并未充分利用以前各层的功能。浅层的原始特征在element-wise操作中丢失，因此以后无法直接使用。他们提出了DenseNet，它通过将输入与剩余输出连接起来而不是逐元素相加来保留浅层功能并改善了信息流：

$$x_{l + 1} = x_l◦f_{l + 1}(x_l,θ)$$

其中◦表示串联。

### 1.2.4. DPN
&emsp;&emsp;Chen等认为，在DenseNet中，大多数浅层的新利用功能都是重复的，并产生了高昂的计算成本。他们结合ResNet和DenseNet的优势，提出了将 $x_l$ 通道分为两部分的双路径网络（Dual Path Network,DPN）： $x^r_l$ 和 $x^d_l$ 。 $x^d_l$ 用于密集连接计算，$x^r_l$ 用于逐元素求和，具有未共享的残差学习分支 $f^d_{l + 1}$ 和 $f^r_{l + 1}$ 。最终结果是两个分支的并置输出：

$$x_{l + 1} =(x^r_l + f^r_{l + 1}(x^r_l，θ_r))◦(x^d_l◦f^d_{l + 1}(x^d_l,θ_d))$$

### 1.2.5. ResNeXt
&emsp;&emsp;基于ResNet，Xie等人提出了ResNeXt，它在保持相当分类精度的同时，大大减少了计算和存储成本。 ResNeXt采用了组卷积层（group convolution layers），它稀疏地连接了特征图通道以降低计算成本。通过增加组数以使计算成本与原始ResNet保持一致，ResNeXt从训练数据中捕获了更丰富的语义特征表示，从而提高了主干精度。

### 1.2.6. MobileNet
&emsp;&emsp;后来，Howard等将坐标设置为等于每个特征图的通道数，并开发了MobileNet。 MobileNet显着降低了计算成本以及参数数量，而不会显着降低分类精度。该模型是专门为在移动平台上使用而设计的。

### 1.2.7. GoogleNet
&emsp;&emsp;除了增加模型深度之外，还进行了一些尝试来探索增加模型宽度以提高学习能力的好处。 Szegedy等提出了带有inception模块的GoogleNet，该模块在给定层的同一特征图上应用了不同比例的卷积核（1×1、3×3和5×5）。这样，它捕获了多尺度特征并将这些特征汇总为输出特征图。后来通过选择卷积核的不同设计，并引入残差块，开发了该模型的更好版本。

### 1.2.8. DetNet
&emsp;&emsp;上面介绍的网络结构都是为图像分类而设计的。通常，将在ImageNet上训练的这些模型用作用于目标检测的模型的初始化。但是，由于分类和检测任务之间可能存在冲突，因此从分类到检测直接应用此预训练模型不是最优的。具体来说：

- i）分类需要较大的感受野，并希望保持空间不变性。因此，应用了多个下采样操作（例如池化层）以降低特征图的分辨率。生成的特征图是低分辨率且空间不变的，并且具有较大的感受野。然而，在检测中，需要高分辨率的空间信息以正确地定位对象；
- ii）分类在单个特征图上进行预测，而检测则需要具有多种表示形式的特征图来以多个比例检测物体。

为了消除这两个任务之间的困难，Li等人介绍了专门为检测而设计的DetNet DetNet保留了高分辨率的特征图，并通过扩大的卷积来进行预测以增加感受野。此外，DetNet在多尺度特征图上检测到对象，从而提供了更丰富的信息。 DetNet在大规模分类数据集上进行了预训练，而网络结构旨在进行检测。

### 1.2.9. Hourglass
&emsp;&emsp;Hourglass网络是另一种体系结构，它不是专门为图像分类而设计的。Hourglass网络最早出现在人体姿势识别任务中，它是具有一系列Hourglass模块的完全卷积结构。Hourglass模块首先通过一系列卷积层或池化层对输入图像进行下采样，然后通过反卷积操作对特征图进行上采样。为避免在下采样阶段丢失信息，在下采样和上采样特征之间使用了跳过连接（skip connection）。Hourglass模块可以捕获本地和全局信息，因此非常适合对象检测。当前，Hourglass网络已广泛用于最新的检测框架中。
