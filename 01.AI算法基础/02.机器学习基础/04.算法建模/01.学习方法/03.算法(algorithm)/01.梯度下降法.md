# 梯度下降法
&emsp;&emsp;梯度下降法（英语：Gradient descent）是一个一阶最优化算法，通常也称为最陡下降法。

&emsp;&emsp;要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点；这个过程则被称为梯度上升法。

## 原理描述
&emsp;&emsp;梯度下降方法基于以下的观察：如果实值函数 ${\displaystyle F(\mathbf {x} )}$ 在点 ${\displaystyle \mathbf {a} }$ 处可微且有定义，那么函数 ${\displaystyle F(\mathbf {x} )}$ 在 ${\displaystyle \mathbf {a} }$ 点沿着梯度相反的方向 ${\displaystyle -\nabla F(\mathbf {a} )}$ 下降最多。

因而，如果

$${\displaystyle \mathbf {b} =\mathbf {a} -\gamma \nabla F(\mathbf {a} )}$$
对于 ${\displaystyle \gamma >0}$ 为一个够小数值时成立，那么 ${\displaystyle F(\mathbf {a} )\geq F(\mathbf {b} )}$ 。

&emsp;&emsp;考虑到这一点，我们可以从函数 ${\displaystyle F}$ 的局部极小值的初始估计 ${\displaystyle \mathbf {x} _{0}}$ 出发，并考虑如下序列 ${\displaystyle \mathbf {x} _{0},\mathbf {x} _{1},\mathbf {x} _{2},\dots }$ 使得：

$${\displaystyle \mathbf {x} _{n+1}=\mathbf {x} _{n}-\gamma _{n}\nabla F(\mathbf {x} _{n}),\ n\geq 0}$$

因此可得到：

$${\displaystyle F(\mathbf {x} _{0})\geq F(\mathbf {x} _{1})\geq F(\mathbf {x} _{2})\geq \cdots ,}$$
如果顺利的话序列 ${\displaystyle (\mathbf {x} _{n})}$ 收敛到期望的局部极小值。注意每次迭代步长 ${\displaystyle \gamma }$ 可以改变。

&emsp;&emsp;下图示例了这一过程，这里假设 ${\displaystyle F}$ 定义在平面上，并且函数图像是一个碗形。蓝色的曲线是等高线（水平集），即函数 ${\displaystyle F}$ 为常数的集合构成的曲线。红色的箭头指向该点梯度的反方向。（一点处的梯度方向与通过该点的等高线垂直）。沿着梯度下降方向，将最终到达碗底，即函数 ${\displaystyle F}$ 局部极小值的点。
<div align=center>
	<img src="images/梯度下降法描述.png">
</div>
<div align=center><font color="gray">图1 梯度下降法的描述</font></div>
<br>
