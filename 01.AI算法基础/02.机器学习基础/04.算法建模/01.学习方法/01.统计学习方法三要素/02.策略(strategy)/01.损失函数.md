# 损失函数
&emsp;&emsp;损失函数（loss function）或代价函数（cost function）是将随机事件或其有关随机变量的取值映射为非负实数以表示该随机事件的“风险”或“损失”的函数。在应用中，损失函数通常作为学习准则与优化问题相联系，即通过最小化损失函数求解和评估模型。在统计学和机器学习中被用于模型的参数估计（parameteric estimation）。

## 0-1损失函数
&emsp;&emsp;0-1损失是指，预测值和目标值不相等为1，否则为0： 
$${\displaystyle L(Y,f(x))={\begin{cases}{1},&Y = f(x) \\\ {0},&{Y \neq f(x)}  \end{cases}}}$$

&emsp;&emsp;该损失函数不考虑预测值和真实值的误差程度，也就是说只要预测错误，预测错误差一点和差很多是一样的。感知机就是用的这种损失函数，但是由于相等这个条件太过严格，我们可以放宽条件，即满足 $|Y−f(X)|<T$ 时认为相等。 

$${\displaystyle L(Y,f(x))={\begin{cases}{1},&|Y - f(x)| \geq 0 \\\ {0},&{|Y - f(x)| < 0}  \end{cases}}}$$

## 绝对值损失函数
&emsp;&emsp;绝对值损失函数：
$$ L(Y,f(x))= |Y -f(x)| $$

## 平方损失函数（squared loss）
&emsp;&emsp;实际结果和观测结果之间差距的平方和，一般用在线性回归中，可以理解为最小二乘法：
$$ L(Y,f(x))= \sum_{i=0}^N{(Y_i -f(x_i))^2} $$

## 对数损失函数（logarithmic loss）
&emsp;&emsp;主要在逻辑回归中使用，样本预测值和实际值的误差符合高斯分布，使用极大似然估计的方法，取对数得到损失函数：
$$ L(Y,P(Y|X))= -\log P(Y|X)$$

损失函数 $L(Y,P(Y|X))$ 是指样本 $X$ 在分类 $Y$ 的情况下，使概率 $P(Y|X)$ 达到最大值。

&emsp;&emsp;经典的对数损失函数包括entropy和softmax，一般在做分类问题的时候使用。回归问题用绝对值损失（拉普拉斯分布时，μ值为中位数）和平方损失（高斯分布时，μ值为均值）。    

## 指数损失函数（Exp-Loss）
&emsp;&emsp;学过Adaboost算法的人都知道，它是前向分步加法算法的特例，是一个加和模型，损失函数就是指数函数。在Adaboost中，经过m此迭代之后，可以得到
$$f_m (x) = f_{m-1}(x) + \alpha_m G_m(x)$$

Adaboost每次迭代时的目的是为了找到最小化下列式子时的参数
$$\arg \min_{\alpha, G} = \sum_{i=1}^{N} exp[-y_{i} (f_{m-1}(x_i) + \alpha G(x_{i}))]$$

而指数损失函数(exp-loss）的标准形式如下:

$$L(y, f(x)) = \exp[-yf(x)]$$

可以看出，Adaboost的目标式子就是指数损失，在给定n个样本的情况下，Adaboost的损失函数为：

$$L(y, f(x)) = \frac{1}{n}\sum_{i=1}^{n}\exp[-y_if(x_i)]$$

## 铰链损失函数（Hinge Loss）
&emsp;&emsp;铰链损失函数主要用在SVM中，Hinge Loss的标准形式为：

$$ L(y,y') = max(0,y*y') $$

$y'$ 是预测值，在-1到+1之间，$y$ 为目标值（-1或+1）。其含义为，$y'$ 的值在-1和+1之间就可以了，并不鼓励 $|y'|>1$ ，即并不鼓励分类器过度自信，让某个正确分类的样本的距离分割线超过1并不会有任何奖励，从而使分类器可以更专注于整体的分类误差。

&emsp;&emsp;在线性支持向量机中，最优化问题可以等价于下列式子：

$$\min_{w,b} \sum_{i}^{N} [1 - y_i(w\cdot x_i + b)]_{+} + \lambda||w||^2 $$

下面来对式子做个变形，令：

$$[1 - y_i(w\cdot x_i + b)]_{+} = \xi_{i}$$

于是，原式就变成了：

$$\min_{w,b}  \ \sum_{i}^{N} \xi_i + \lambda||w||^2 $$                        
