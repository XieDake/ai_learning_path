# 超参数
&emsp;&emsp;在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。

## 定义
&emsp;&emsp;在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数。 相反，其他参数的值通过训练得出。

超参数有以下性质：
- 定义关于模型的更高层次的概念，如复杂性或学习能力;
- 不能直接从标准模型培训过程中的数据中学习，需要预先定义;
- 可以通过设置不同的值，训练不同的模型和选择更好的测试值来决定。

超参数的一些示例：
- 树的数量或树的深度
- 矩阵分解中潜在因素的数量
- 学习率（多种模式）
- 深层神经网络隐藏层数
- k均值聚类中的簇数

## 超参数的优化问题
&emsp;&emsp;超参数优化或模型选择是为学习算法选择一组最优超参数时的问题，通常目的是优化算法在独立数据集上的性能的度量。通常使用交叉验证来估计这种泛化性能。超参数优化与实际的学习问题形成对比，这些问题通常也被转化为优化问题，但是优化了训练集上的损失函数。实际上，学习算法学习可以很好地建模/重建输入的参数，而超参数优化则是确保模型不会像通过正则化一样通过调整来过滤其数据。

### 网格搜索
&emsp;&emsp;执行超参数优化的传统方法是网格搜索或参数扫描，这仅仅是通过学习算法的超参数空间的手动指定子集的详尽搜索。网格搜索算法必须由某些性能度量指导，通常通过训练集合上的交叉验证或对被保留验证集进行评估来衡量。

&emsp;&emsp;由于机器学习者的参数空间可能包括某些参数的实值或无界值空间，因此在应用网格搜索之前可能需要手动设置边界和离散化。

### 贝叶斯优化
&emsp;&emsp;贝叶斯优化包括从超参数值到在验证集上评估的目标的功能的统计模型。直观上，该方法假设有一些平滑但嘈杂的功能，作为从超参数到目标的映射。在贝叶斯优化中，一个目的是收集观察结果，以便尽可能少地显示机器学习模型的次数，同时尽可能多地显示关于该功能的信息，特别是最佳位置。贝叶斯优化依赖于假设一个非常普遍的先验函数，当与观察到的超参数值和相应的输出结合时，产生函数分布。该方法通过迭代地选择超参数来观察（实验运行），以抛售（结果最不确定的超参数）和利用（预期具有良好结果的超参数）的方式。实际上，贝叶斯优化已经被证明，因为在实验的质量运行之前，能够对网格搜索和随机搜索进行更少的实验获得更好的结果。

### 随机搜索
&emsp;&emsp;由于网格搜索是一种穷尽且潜在昂贵的方法，因此已经提出了几种替代方案。特别地，已经发现，简单地对参数设置进行固定次数的随机搜索，比在穷举搜索中的高维空间更有效。这是因为事实证明，一些超参数不会显着影响损失。因此，随机分散的数据给出了比最终不影响损失的参数的详尽搜索更多的“纹理”数据。

### 基于梯度的优化
&emsp;&emsp;对于特定的学习算法，可以计算相对于超参数的梯度，然后使用梯度下降优化超参数。这些技术的第一次使用集中在神经网络从那时起，这些方法已经扩展到其他模型，如支持向量机或逻辑回归。
