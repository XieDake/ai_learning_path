# Softmax与多元逻辑回归
&emsp;&emsp;针对分类问题，softmax函数常被用来定义神经网络的损失函数。根据机器学习的理论，二元逻辑回归的模型公式可以写为如下的形式：
$$P(y=1)=\frac 1 {1+e^{-XW^T+b}}$$

在公式中，对分子、分母同时乘以 ${e^{XW_1^T+b_1}}$，其中：
$$W_0 = W_1 - W$$
$$b_0 = b_1 - b$$
得到:
$$P(y=1)=\frac {e^{XW_1^T+b_1}} {e^{XW_0^T+b_0}+e^{XW_1^T+b_1}}$$
$$P(y=0)=\frac {e^{XW_0^T+b_0}} {e^{XW_0^T+b_0}+e^{XW_1^T+b_1}}$$

对于多元逻辑回归的模型可以表示为如下的形式：
$$P(y=l)=\frac {e^{XW_l^T+b_l}} {\displaystyle \sum_{c=1}^{C} {e^{XW_c^T+b_c}}}$$
其中 $C$ 为类别数, $l$ 为类别。

公式中的函数其实就是softmax函数（softmax function），记为 $\sigma(Z)$ 。
