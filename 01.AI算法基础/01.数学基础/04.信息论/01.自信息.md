# 简介
&emsp;&emsp;在信息论中，自信息（英语：self-information），由克劳德·香农提出，是与概率空间中的单一事件或离散随机变量的值相关的信息量的量度。它用信息的单位表示，例如 bit、nat或是hart，使用哪个单位取决于在计算中使用的对数的底。自信息的期望值就是信息论中的熵，它反映了随机变量采样时的平均不确定程度。

# 定义
&emsp;&emsp;当信息被拥有它的实体传递给接收它的实体时，仅当接收实体不知道信息的先验知识时信息才得到传递。如果接收实体事先知道了消息的内容，这条消息所传递的信息量就是0。只有当接收实体对消息对先验知识少于100%时，消息才真正传递信息。

&emsp;&emsp;因此，一个随机产生的事件 ${\displaystyle \omega _{n}}$ 所包含的自信息数量，只与事件发生的几率相关。事件发生的几率越低，在事件真的发生时，接收到的信息中，包含的自信息越大。

&emsp;&emsp;因此 ${\displaystyle \omega _{n}}$ 的自信息量 ${\displaystyle \operatorname {I} (\omega _{n})=f(\operatorname {P} (\omega _{n}))}$。如果 ${\displaystyle \operatorname {P} (\omega _{n})=1}$ ，那么 ${\displaystyle \operatorname {I} (\omega _{n})=0}$ 。如果 ${\displaystyle \operatorname {P} (\omega _{n})<1}$ ，那么 ${\displaystyle \operatorname {I} (\omega _{n})>0}$ 。

&emsp;&emsp;此外，根据定义，自信息的量度是非负的而且是可加的。如果事件 ${\displaystyle C}$ 是两个独立事件 ${\displaystyle A}$ 和 ${\displaystyle B}$ 的交集，那么宣告 ${\displaystyle C}$ 发生的信息量就等于分别宣告事件 ${\displaystyle A}$ 和事件 ${\displaystyle B}$ 的信息量的和：
$${\displaystyle \operatorname {I} (C)=\operatorname {I} (A\cap B)=\operatorname {I} (A)+\operatorname {I} (B)}$$
因为 ${\displaystyle A}$ 和 ${\displaystyle B}$ 是独立事件，所以 ${\displaystyle C}$ 的概率为:
$${\displaystyle \operatorname {P} (C)=\operatorname {P} (A\cap B)=\operatorname {P} (A)\cdot \operatorname {P} (B)}$$
应用函数 ${\displaystyle f(\cdot )}$ 会得到:
$${\displaystyle {\begin{aligned}\operatorname {I} (C)&=\operatorname {I} (A)+\operatorname {I} (B)\\f(\operatorname {P} (C))&=f(\operatorname {P} (A))+f(\operatorname {P} (B))\\&=f{\big (}\operatorname {P} (A)\cdot \operatorname {P} (B){\big )}\\\end{aligned}}}$$
所以函数 ${\displaystyle f(\cdot )}$ 有性质:
$${\displaystyle f(x\cdot y)=f(x)+f(y)}$$
而对数函数正好有这个性质，不同的底的对数函数之间的区别只差一个常数:
$${\displaystyle f(x)=K\log(x)}$$
由于事件的概率总是在0和1之间，而信息量必须是非负的，所以 ${\displaystyle K<0}$。

&emsp;&emsp;考虑到这些性质，假设事件 ${\displaystyle \omega _{n}}$ 发生的几率是 ${\displaystyle P(\omega _{n})}$ ，自信息 ${\displaystyle I(\omega _{n})}$ 的定义就是:

$${\displaystyle \operatorname {I} (\omega _{n})=-\log(\operatorname {P} (\omega _{n}))=\log \left({\frac {1}{\operatorname {P} (\omega _{n})}}\right)}$$
事件 ${\displaystyle \omega _{n}}$ 的概率越小, 它发生后的自信息量越大。

&emsp;&emsp;此定义符合上述条件。在上面的定义中，没有指定的对数的基底：如果以 2 为底，单位是bit。当使用以 e 为底的对数时，单位将是 nat。对于基底为 10 的对数，单位是 hart。

&emsp;&emsp;信息量的大小不同于信息作用的大小，这不是同一概念。信息量只表明不确定性的减少程度，至于对接收者来说，所获得的信息可能事关重大，也可能无足轻重，这是信息作用的大小。

# 和熵的关系
&emsp;&emsp;熵是离散随机变量的自信息的期望值。但有时候熵也会被称作是随机变量的自信息，可能是因为熵满足 ${\displaystyle \operatorname {H} (X)=\operatorname {I} (X;X)}$ ，而 ${\displaystyle \operatorname {I} (X;X)}$ 是 ${\displaystyle X}$ 与它自己的互信息。
