# 简介
&emsp;&emsp;在概率论和信息论中，两个随机变量的互信息（mutual Information，简称MI）或转移信息（transinformation）是变量间相互依赖性的量度。不同于相关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布 $p(X,Y)$ 和分解的边缘分布的乘积 $p(X)p(Y)$ 的相似程度。互信息是点间互信息（英语：Pointwise mutual information）（PMI）的期望值。互信息最常用的单位是bit。

# 定义
&emsp;&emsp;一般地，两个离散随机变量 X 和 Y 的互信息可以定义为：
$${\displaystyle I(X;Y)=\sum _{y\in Y}\sum _{x\in X}p(x,y)\log {\left({\frac {p(x,y)}{p(x)\,p(y)}}\right)},\,\!}$$
其中 p(x, y) 是 X 和 Y 的联合概率分布函数，而 ${\displaystyle p(x)}$ 和 ${\displaystyle p(y)}$ 分别是 $X$ 和 $Y$ 的边缘概率分布函数。

&emsp;&emsp;在连续随机变量的情形下，求和被替换成了二重定积分：
$${\displaystyle I(X;Y)=\int _{Y}\int _{X}p(x,y)\log {\left({\frac {p(x,y)}{p(x)\,p(y)}}\right)}\;dx\,dy\,}$$
其中 $p(x, y)$ 当前是 $X$ 和 $Y$ 的联合概率密度函数，而 ${\displaystyle p(x)}$ 和 ${\displaystyle p(y)}$ 分别是 $X$ 和 $Y$ 的边缘概率密度函数。

&emsp;&emsp;如果对数以 2 为基底，互信息的单位是bit。

&emsp;&emsp;直观上，互信息度量 X 和 Y 共享的信息：它度量知道这两个变量其中一个，对另一个不确定度减少的程度。例如，如果 X 和 Y 相互独立，则知道 X 不对 Y 提供任何信息，反之亦然，所以它们的互信息为零。在另一个极端，如果 X 是 Y 的一个确定性函数，且 Y 也是 X 的一个确定性函数，那么传递的所有信息被 X 和 Y 共享：知道 X 决定 Y 的值，反之亦然。因此，在此情形互信息与 Y（或 X）单独包含的不确定度相同，称作 Y（或 X）的熵。而且，这个互信息与 X 的熵和 Y 的熵相同。（这种情形的一个非常特殊的情况是当 X 和 Y 为相同随机变量时。）

&emsp;&emsp;互信息是 X 和 Y 的联合分布相对于假定 X 和 Y 独立情况下的联合分布之间的内在依赖性。 于是互信息以下面方式度量依赖性：I(X; Y) = 0 当且仅当 X 和 Y 为独立随机变量。从一个方向很容易看出：当 X 和 Y 独立时，p(x,y) = p(x) p(y)，因此：

$${\displaystyle \log {\left({\frac {p(x,y)}{p(x)\,p(y)}}\right)}=\log 1=0.\,\!}$$
此外，互信息是非负的（即 ${\displaystyle I(X;Y)\geq 0}$），而且是对称的（英语：Symmetric function）（即 ${\displaystyle I(X;Y)=I(Y;X)}$ ）。
