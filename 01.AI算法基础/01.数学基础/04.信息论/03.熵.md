# 简介
&emsp;&emsp;熵的概念最早起源于物理学，用于度量一个热力学系统的无序程度。在信息论里面，熵是对不确定性的测量。但是在信息世界，熵越高，则能传输越多的信息，熵越低，则意味着传输的信息越少。\
&emsp;&emsp;简单来讲：信息熵表示不确定性的度量。

## 举例：英文文本
&emsp;&emsp;英语文本数据流的熵比较低，因为英语很容易读懂，也就是说很容易被预测。即便我们不知道下一段英语文字是什么内容，但是我们能很容易地预测，比如，字母e总是比字母z多，或者qu字母组合的可能性总是超过q与任何其它字母的组合。如果未经压缩，一段英文文本的每个字母需要8个比特来编码，但是实际上英文文本的熵大概只有4.7比特。

## 举例：无损压缩
&emsp;&emsp;如果压缩是无损的，即通过解压缩可以百分之百地恢复初始的消息内容，那么压缩后的消息携带的信息和未压缩的原始消息是一样的多。而压缩后的消息可以通过较少的比特传递，因此压缩消息的每个比特能携带更多的信息，也就是说压缩信息的熵更加高。熵更高意味着比较难于预测压缩消息携带的信息，原因在于压缩消息里面没有冗余，即每个比特的消息携带了一个比特的信息。香农的信息理论揭示了，任何无损压缩技术不可能让一比特的消息携带超过一比特的信息。消息的熵乘以消息的长度决定了消息可以携带多少信息。\
香农的信息理论同时揭示了，任何无损压缩技术不可能缩短任何消息。根据鸽笼原理，如果有一些消息变短，则至少有一条消息变长。在实际使用中，由于我们通常只关注于压缩特定的某一类消息，所以这通常不是问题。例如英语文档和随机文字，数字照片和噪音，都是不同类型的。所以如果一个压缩算法会将某些不太可能出现的，或者非目标类型的消息变得更大，通常是无关紧要的。但是，在我们的日常使用中，如果去压缩已经压缩过的数据，仍会出现问题。例如，将一个已经是FLAC格式的音乐文件压缩为ZIP文件很难使它占用的空间变小。

## 举例：硬币
&emsp;&emsp;如果有一枚理想的硬币，其出现正面和反面的机会相等，则抛硬币事件的熵等于其能够达到的最大值。我们无法知道下一个硬币抛掷的结果是什么，因此每一次抛硬币都是不可预测的。因此，使用一枚正常硬币进行若干次抛掷，这个事件的熵是一比特，因为结果不外乎两个——正面或者反面，可以表示为0, 1编码，而且两个结果彼此之间相互独立。若进行n次独立实验，则熵为n，因为可以用长度为n的比特流表示。但是如果一枚硬币的两面完全相同，那个这个系列抛硬币事件的熵等于零，因为结果能被准确预测。现实世界里，我们收集到的数据的熵介于上面两种情况之间。

# 信息熵
&emsp;&emsp;香农把随机变量X的熵值 $Η$定义如下，其值域为 ${\{x_1, \ldots, x_n\}}$：
$${\displaystyle \mathrm {H} (X)=\mathrm {E} [\mathrm {I} (X)]=\mathrm {E} [-\ln(\mathrm {P} (X))]}$$
其中：
- $I(X)$ 是 $X$ 的信息量（又称为自信息）;
- $P$ 为 $X$ 的概率质量函数（probability mass function）;
- ${E}$ 为期望函数。

当取有限的样本时，熵的公式可以表示为：
$${\displaystyle \mathrm {H} (X)=\sum _{i}{\mathrm {P} (x_{i})\,\mathrm {I} (x_{i})}=-\sum _{i}{\mathrm {P} (x_{i})\log _{b}\mathrm {P} (x_{i})},}$$

在这里b是对数所使用的底，通常是2,自然常数e，或是10。当b = 2，熵的单位是bit；当b = e，熵的单位是nat；而当b = 10,熵的单位是Hart。

# 联合熵
&emsp;&emsp;联合熵衡量的是一对随机变量 $X$ 和 $Y$ 的不确定性。

&emsp;&emsp;设 $X$ ，$Y$ 值域分别为 ${\{x_1, \ldots, x_n\}}$、${\{y_1, \ldots, y_n\}}$，联合熵定义如下：
$${\displaystyle \mathrm {H} (X,Y)=-\sum _{i,j}p(x_{i},y_{j})\log p(x_{i},y_{j})}$$

&emsp;&emsp;其中 $p(x_i, y_j)$ 为 $X = x_i$ 且 $Y = y_j$ 时的概率。这个量应当理解为你知道Y的值前提下随机变量 X 的随机性的量。

# 条件熵
&emsp;&emsp;条件熵衡量的是一个随机变量X已知的情况下，另一个随便变量Y的不确定性。

&emsp;&emsp;设 $X$ ，$Y$ 值域分别为 ${\{x_1, \ldots, x_n\}}$、${\{y_1, \ldots, y_n\}}$，条件熵定义如下：
$${\displaystyle \mathrm {H} (X|Y)=-\sum _{i,j}p(x_{i},y_{j})\log {\frac {p(x_{i},y_{j})}{p(y_{j})}}}$$

# 相对熵(KL散度)
&emsp;&emsp;KL散度（Kullback-Leibler divergence，简称KLD），在信息系统中称为相对熵（relative entropy），在连续时间序列中称为随机性（randomness），在统计模型推断中称为信息增益（information gain），也称信息散度（information divergence）。

&emsp;&emsp;KL散度是两个概率分布P和Q差别的非对称性的度量。 KL散度是用来度量使用基于Q的分布来编码服从P的分布的样本所需的额外的平均比特数。典型情况下，P表示数据的真实分布，Q表示数据的理论分布、估计的模型分布、或P的近似分布。

&emsp;&emsp;对于离散随机变量，其概率分布 $P$ 和 $Q$ 的KL散度可按下式定义为：
$${\displaystyle D_{\mathrm {KL} }(P\|Q)=-\sum _{i}P(i)\ln {\frac {Q(i)}{P(i)}}.\!}$$
等价于
$$ {\displaystyle D_{\mathrm {KL} }(P\|Q)=\sum _{i}P(i)\ln {\frac {P(i)}{Q(i)}}.\!}$$

即按概率P求得的P和Q的对数商的平均值。

&emsp;&emsp;简单的讲相对熵衡量的是相同事件空间里两个概率分布（函数）的差异程度。当两个概率分布完全相同时，它们的相对熵为0,当它们的差异增加时，相对熵就会增加。

# 交叉熵
&emsp;&emsp;在信息论中，基于相同事件测度的两个概率分布 ${\displaystyle p}$ 和 ${\displaystyle q}$ 的交叉熵是指，当基于一个“非自然”（相对于“真实”分布 ${\displaystyle p}$ 而言）的概率分布 ${\displaystyle q}$ 进行编码时，在事件集合中唯一标识一个事件所需要的平均比特数（bit）。

&emsp;&emsp;给定两个概率分布 ${\displaystyle p}$ 和 ${\displaystyle q}$ ， ${\displaystyle p}$ 相对于 ${\displaystyle q}$ 的交叉熵定义为：
$${\displaystyle H(p,q)=\operatorname {E} _{p}[-\log q]=H(p)+D_{\mathrm {KL} }(p\|q),\!}$$
其中 ${\displaystyle H(p)}$ 是 ${\displaystyle p}$ 的熵， ${\displaystyle D_{\mathrm {KL} }(p\|q)}$ 是从 ${\displaystyle p}$ 与 ${\displaystyle q}$ 的KL散度(也被称为p相对于q的相对熵)。

&emsp;&emsp;对于离散分布 ${\displaystyle p}$ 和 ${\displaystyle q}$ ，这意味着：
$${\displaystyle H(p,q)=-\sum _{x}p(x)\,\log q(x).\!}$$

&emsp;&emsp;对于连续分布也是类似的。我们假设 ${\displaystyle p}$ 和 ${\displaystyle q}$ 在测度 ${\displaystyle r}$ 上是绝对连续的(通常 ${\displaystyle r}$ 是Lebesgue measure on a Borel σ-algebra)。设 ${\displaystyle P}$ 和 ${\displaystyle Q}$ 分别为 ${\displaystyle p}$ 的 ${\displaystyle q}$ 在测度 ${\displaystyle r}$ 上概率密度函数。则 ${\displaystyle -\int _{X}P(x)\,\log Q(x)\,dr(x)=\operatorname {E} _{p}[-\log Q]}$。
